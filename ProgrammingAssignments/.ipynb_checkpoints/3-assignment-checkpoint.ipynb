{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "In the third assignment you will cluster hand-written digits using k-means. This is an unsupervised method and will therefore differ a bit from the previous assignments. \n",
    "You will also have to analyse the behavior of K-means for different parameter settings and initializations.\n",
    "\n",
    "The assignment follows Andrew Ng's explanation of K-means and (re)watching his videos could be useful.\n",
    "\n",
    "Publish your notebook (ipynb file) to your repository on Github. If you have any questions, please email Gosia and Rein and don't forget to commit the most recent version of you assignment on Github so that we can see your code.\n",
    "\n",
    "Note: all code should be clear, add comments where necessary (especially if your code is not straightforward). You are free to discuss the assignment among each other, but stick to sharing ideas, not code.\n",
    "\n",
    "### Deadline: December 1, 23:59\n",
    "\n",
    "Do not hand in any other files, the Notebook should contain all your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is the same as with logistic regression. However, you don't need to use the class labels as K-means is an unsupervised method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits(n_class=10)\n",
    "\n",
    "#Create two rows with numbers\n",
    "firstrow = np.hstack(digits.images[:5,:,:])\n",
    "secondrow = np.hstack(digits.images[5:10,:,:])\n",
    "\n",
    "plt.gray()\n",
    "plt.axis('off')\n",
    "\n",
    "#Show both rows at the same time\n",
    "plt.imshow(np.vstack((firstrow,secondrow)))\n",
    "\n",
    "#Show both rows at the same time using nearest-neighbor interpolation (pixelated image)\n",
    "#plt.imshow(np.vstack((firstrow,secondrow)),  interpolation=\"nearest\")\n",
    "\n",
    "print \"The numbers shown are: \\n\", np.vstack((digits.target[:5], digits.target[5:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize functions\n",
    "\n",
    "In order to make it more visible what K-means is learning, we provide two visualization functions. The first function takes the assignment of each digit to each cluster and shows what the digits of the cluster look like. If your algorithm works well then the digits should look alike!\n",
    "\n",
    "The second function visualizes the actual cluster centroids. It is expected that you see something that resembles the original digit, but don't be surprised if one of the results is a combination, e.g. a zero and eight at the same time.\n",
    "\n",
    "K-means is a very simple algorithm and is not likely to get an optimal result, therefore some of the centroids will be just noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#With below code you can visualize the data clusters\n",
    "#The input is a vector c (with all the datapoints assigned to a cluster)\n",
    "#this function will show a bunch of numbers assigned to that cluster.\n",
    "#It will skip the cluster if there are not enough images in it.\n",
    "\n",
    "#You can use this code after you have finished implementing everything!\n",
    "def visualize(best_c):\n",
    "    for i in range(10):\n",
    "        print \"There are \", np.sum(c == i), \" elements in cluster \", i\n",
    "    \n",
    "    for cluster in range(10):\n",
    "        #Create two rows with numbers\n",
    "        filtered = digits.images[c == cluster,:,:]\n",
    "\n",
    "        if filtered.shape[0] < 5:\n",
    "            print \"not enough items in cluster \", cluster\n",
    "            continue\n",
    "\n",
    "        rows = np.floor(filtered.shape[0]/10)\n",
    "        image_rows = []\n",
    "\n",
    "        for i in range(min(5,int(rows))):\n",
    "            image_row = np.hstack(filtered[i*10:(i+1)*10,:,:])\n",
    "            image_rows.append(image_row)\n",
    "\n",
    "        plt.gray()\n",
    "        plt.axis('off')\n",
    "\n",
    "        #Show both rows at the same time\n",
    "        plt.imshow(np.vstack(image_rows))\n",
    "        plt.show()\n",
    "\n",
    "        print \"The cluster shown above is: \", cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function visualizes the centroids. \n",
    "# It takes as input the matrix mu with the cluster centroids and shows an image.\n",
    "\n",
    "#You can use this code after you have finished implementing everything!\n",
    "def visualize_centroids(best_mu):\n",
    "    mu = best_mu.T.reshape(10,8,8)\n",
    "    \n",
    "    #Create two rows with numbers\n",
    "    firstrow = np.hstack(mu[:5,:,:])\n",
    "    secondrow = np.hstack(mu[5:,:,:])\n",
    "\n",
    "    plt.gray()\n",
    "    plt.axis('off')\n",
    "\n",
    "    #Show both rows at the same time\n",
    "    plt.imshow(np.vstack((firstrow,secondrow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The implementation\n",
    "\n",
    "You have to implement the next three functions and fill in the body of the loop in order to create a correct implementation of k-means. Please follow Andrew Ng's description and the instructions that come with the functions closely. \n",
    "\n",
    "Think about the problem one step at a time. First focus on implementing the functions and only then start thinking about how they all come together. The skeleton code will guide you to make sure that it will come together. \n",
    "\n",
    "Grading:\n",
    "- Working version of compute_centroid, update_centroid and cost_function (0.5 point each)\n",
    "- A working triple loop that successfully learns and stores the best result (3 points)\n",
    "- Correct use of both visualization functions on your best result and an explanation how you got to the best result and why it is necessary to have many initializations (1.5 points)\n",
    "\n",
    "## How was the best result obtained?\n",
    "\n",
    "The best result was obtained using many different initialisations of cluster centroids, which eventually leads to an initialisation that allows a fairly efficient clustering of the datapoints. The efficient clustering of each datapoints was evaluated using the cost function which calculates the distance of each datapoint. The smaller the distance, the better the fit of the cluster centroid to the data. \n",
    "\n",
    "## Why is it necessary to have many initialisations?\n",
    "\n",
    "It is necessary to have many initialisations because a single initialisation can result in neighbouring clusters not being able to discriminate the datapoints efficiently, or too distant to allow aberrant clustering. Therefore, many different iterations need to be performed to find the right initialisation values to cluster the data with a minimal cost function value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First write a function that takes as input 1 image x and the cluster centroids mu. \n",
    "#It returns a number that is the closest centroid (index of the closest centroid)\n",
    "def compute_centroid(x,mu):\n",
    "    #mu will be of size 64 by k\n",
    "    #x will be of size 64 (need to reshape to 64 by 1!)\n",
    "    #You can subtract x from mu, and numpy will \"broadcast\" over the columns, result is 64 by k\n",
    "    #Then you can take the norm over the columns (give option axis = 0)\n",
    "    #return the index of the closest\n",
    "    return argmin(np.linalg.norm(mu - np.reshape(x, (64, 1)), axis=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Second write a function that takes as input:\n",
    "#a cluster number\n",
    "#the cluster centroids\n",
    "#all the datapoints assigned to that cluster\n",
    "#It returns an updated mu matrix for that cluster number\n",
    "def update_centroid(centroid, mu, X_c):\n",
    "    #you have to average over the columns in X_c and change only 1 specific column in mu\n",
    "    mu[:, centroid] = np.mean(X_c, axis=0)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now write the cost function. This will be exactly the same as the compute_centroid\n",
    "#However it will return the actual minimum and not the index of the centroid\n",
    "def cost_function(x,mu):\n",
    "    return min(np.linalg.norm(mu - np.reshape(x, (64, 1)), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Now you are going to link it all together again\n",
    "\n",
    "#The amount of iterations\n",
    "initialisation_iterations = 100\n",
    "iterations = 10\n",
    "\n",
    "#define the amount of clusters k\n",
    "k = 10\n",
    "\n",
    "x = np.reshape(digits.images[:1500],(1500,64))\n",
    "x_test = np.reshape(digits.images[1500:],(297,64))\n",
    "\n",
    "#This time the code constists of three layers of loops:\n",
    "#The first loop is for different initialisations of mu\n",
    "#The second loop is for the amount of iterations of the k means algorithm\n",
    "#The third loop actually consists of two loops:\n",
    "    #Loop 1 updates the centroid of all the clusters\n",
    "    #Loop 2 assigns a new cluster to each datapoint\n",
    "\n",
    "cost_min_progression = []\n",
    "\n",
    "for h in range(initialisation_iterations):\n",
    "    #mu is the centroid matrix, initialized by sampling from a uniform distribution (shape is 64 by k!)\n",
    "    mu = np.random.uniform(0,16,(64,k))\n",
    "    \n",
    "    #c is the vector that assigns each digit to a centroid\n",
    "    #initialized with integers between 0 and 10, the shape is 1500 by nothing\n",
    "    c = np.random.randint(0,k,(1500))\n",
    "    cost_minimal = 0\n",
    "    cost = []\n",
    "    #Initiations \n",
    "    for j in range(iterations):\n",
    "         \n",
    "        #Loop 1 updates the centroid of all the clusters. \n",
    "        #As previously stated, in the first loop we update the cluster centroid positions by feeding in (1) the index of the\n",
    "        #cluster centroid to which the datapoint has been assigned (a number ranging from 1 to k), (2) the current cluster \n",
    "        #centroid positions (a vector of dimension 64 by 1), (3) the of all the datapoints belonging to a particular cluster\n",
    "        #(a vector of dimension 64 by n datapoints belonging to the cluster)into the update_centroid function.\n",
    "        for i in range(k):\n",
    "            mu = update_centroid(i,mu,x[c==i].T)\n",
    "            \n",
    "        #Loop 2 assigns a new cluster to each datapoint.\n",
    "        #After having updated the coordinates of each cluster, we can reassign each datapoint to its closest cluster by \n",
    "        #feeding in (1) the datapoints (that is all the datapoints belonging to the training set x, a vector of 64 by n=1500 \n",
    "        #datapoints) (2) the cluster centroid  coordinates (a 64 by 10) into the compute_centroid function.\n",
    "        for i in range(x.shape[0]):\n",
    "            c[i] = compute_centroid(x[i,:],mu)\n",
    "        \n",
    "        #Saves the temporary cost of the iterations in the cost function vector containing the values from the previous \n",
    "        #iterations.\n",
    "        temp_cost = sum([cost_function(x[i,:],mu) for i in range(x.shape[0])])/x.shape[0]\n",
    "        cost.append(temp_cost)       \n",
    "        \n",
    "    #The following condition updates the cluster centroids initialisation if appropriately\n",
    "    #clustered (i.e. if the cost function is lower than previous iterations). \n",
    "    if temp_cost == min(cost) or cost_minimal == 0:\n",
    "        min_cost = temp_cost\n",
    "        best_c = c\n",
    "        best_mu = mu\n",
    "        \n",
    "#Data visualisation\n",
    "print \"The vector x was clustered as follows:\"\n",
    "visualize(best_c)\n",
    "print \"The best initialisations of the cluster centroids were found to be as follows:\"\n",
    "visualize_centroids(best_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "There is a special \"trick\" you can do to make sure all clusters are used. You can do this by initializing the cluster centroids as the first ten digits. Luckily, the dataset x has as first 10 elements the first 10 digits.\n",
    "\n",
    "Take the code with the three loops, remove the outer loop since you already know the \"best\" initialization and use the below line of code to initialize mu.\n",
    "\n",
    "To show:\n",
    "- Visualize the resulting centroids (Do they still resemble the digits?)\n",
    "- Plot the cost over iterations and compare this to the original initialization\n",
    "\n",
    "Max 0.5 points extra, your final amount of points cannot be more than 6.\n",
    "\n",
    "Note: only attempt this if you're confident you got everything working.\n",
    "\n",
    "## Notes on below code\n",
    "\n",
    "We see that the cost function doesn't change when we use the specified initialisation. This is because the cluster centroids are already initialised at the right coordinates. Therefore a big advantage of this procedure is that it is less computationally expensive as less calculations are required to obtain a good result. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#You should initialize mu as:\n",
    "mu = x[:10,:].T\n",
    "\n",
    "#Since we have manually initialised the cluster centroids coordinates in relevant locations, there is no need to run \n",
    "#multiple k-means algorithms. Therefore we can get rid of the outermost loop we had in the previous algorithm. \n",
    "\n",
    "#Here we, again, randomly assign a data point to a cluster number by sampling from a uniform distribution, as we did\n",
    "#in the previous implementation\n",
    "c = np.random.randint(0, k, (1500))\n",
    "\n",
    "#Creating a vector to store the cost values after each iteration. \n",
    "cost_bonus = []\n",
    "\n",
    "#Loop to iterate the two-step k-means algorithm.\n",
    "for j in range(iterations):\n",
    "    \n",
    "    #After having updated the coordinates of each cluster, we can reassign each datapoint to its closest cluster by \n",
    "    #feeding in (1) the datapoints (that is all the datapoints belonging to the training set x, a vector of 64 by n=1500 \n",
    "    #datapoints) (2) the cluster centroid  coordinates (a 64 by 10).\n",
    "    for i in range(x.shape[0]):\n",
    "        c[i] = compute_centroid(x[i,:], mu)\n",
    "        \n",
    "    #As previously stated, in the first loop we update the cluster centroid positions by feeding in (1) the index of the\n",
    "    #cluster centroid to which the datapoint has been assigned (a number ranging from 1 to k), (2) the current cluster \n",
    "    #centroid positions (a vector of dimension 64 by 1), (3) the of all the datapoints belonging to a particular cluster\n",
    "    #(a vector of dimension 64 by n datapoints belonging to the cluster)into the update_centroid function.\n",
    "    for i in range(k):\n",
    "        mu = update_centroid(i, mu, x[c==1].T)\n",
    "    \n",
    "    #Append current cost value after the iteration of the k-means algorithm. \n",
    "    cost_bonus.append(sum([cost_function(x[i,:], mu) for i in range(x.shape[0])])/x.shape[0])\n",
    "        \n",
    "plt.plot(cost_bonus, linestyle='dashed', color='b')\n",
    "plt.plot(cost, color='r')\n",
    "plt.legend(['First 10 digits initalisation', 'Randomly initialised'])\n",
    "plt.suptitle('Comparative cost functions')\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Nulber of iterations of k-means')\n",
    "plt.show()\n",
    "\n",
    "# visualize centroids\n",
    "print \"The centroids were initialised the following way:\"\n",
    "visualize_centroids(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
